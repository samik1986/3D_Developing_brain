{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samik1986/3D_Developing_brain/blob/main/MEDIAR_(model1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biuER5L5P_bp"
      },
      "source": [
        "# MEDIAR Prediction Tutorial\n",
        "\n",
        "This tutorial guides how to conduct prediction using pretrained models with MEDIAR pipeline. Note that this is a inline version of running `predict.py` in MEDIAR repo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00LjatqCPZf6"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## 1. Setup\n",
        "\n",
        "Clone Github repository and install dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wC7RDZmIDTR",
        "outputId": "119c3b6b-0b3a-4900-f28d-f1fa3a20a91c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Mediar_4_devBrain' already exists and is not an empty directory.\n",
            "/content/Mediar_4_devBrain\n",
            "Requirement already satisfied: fastremap==1.14.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (1.14.1)\n",
            "Requirement already satisfied: monai==1.3.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: numba==0.57.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (0.57.1)\n",
            "Requirement already satisfied: numpy==1.24.3 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (1.24.3)\n",
            "Requirement already satisfied: pandas==2.0.3 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (2.0.3)\n",
            "Requirement already satisfied: pytz==2023.3.post1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (2023.3.post1)\n",
            "Requirement already satisfied: scipy==1.12.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (1.12.0)\n",
            "Collecting segmentation_models_pytorch==0.3.3 (from -r requirements.txt (line 8))\n",
            "  Using cached segmentation_models_pytorch-0.3.3-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: tifffile==2023.4.12 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 9)) (2023.4.12)\n",
            "Requirement already satisfied: torch==2.1.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 10)) (2.1.2)\n",
            "Requirement already satisfied: tqdm==4.65.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 11)) (4.65.0)\n",
            "Requirement already satisfied: wandb==0.16.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 12)) (0.16.2)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 13)) (0.25.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 14)) (3.10.0)\n",
            "Requirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba==0.57.1->-r requirements.txt (line 3)) (0.40.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.0.3->-r requirements.txt (line 5)) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.0.3->-r requirements.txt (line 5)) (2025.2)\n",
            "Requirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from segmentation_models_pytorch==0.3.3->-r requirements.txt (line 8)) (0.16.2)\n",
            "Requirement already satisfied: pretrainedmodels==0.7.4 in /usr/local/lib/python3.11/dist-packages (from segmentation_models_pytorch==0.3.3->-r requirements.txt (line 8)) (0.7.4)\n",
            "Requirement already satisfied: efficientnet-pytorch==0.7.1 in /usr/local/lib/python3.11/dist-packages (from segmentation_models_pytorch==0.3.3->-r requirements.txt (line 8)) (0.7.1)\n",
            "Collecting timm==0.9.2 (from segmentation_models_pytorch==0.3.3->-r requirements.txt (line 8))\n",
            "  Using cached timm-0.9.2-py3-none-any.whl.metadata (68 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from segmentation_models_pytorch==0.3.3->-r requirements.txt (line 8)) (11.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2->-r requirements.txt (line 10)) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2->-r requirements.txt (line 10)) (4.14.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2->-r requirements.txt (line 10)) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2->-r requirements.txt (line 10)) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2->-r requirements.txt (line 10)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2->-r requirements.txt (line 10)) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2->-r requirements.txt (line 10)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2->-r requirements.txt (line 10)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2->-r requirements.txt (line 10)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2->-r requirements.txt (line 10)) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2->-r requirements.txt (line 10)) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2->-r requirements.txt (line 10)) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2->-r requirements.txt (line 10)) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2->-r requirements.txt (line 10)) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2->-r requirements.txt (line 10)) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2->-r requirements.txt (line 10)) (2.18.1)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2->-r requirements.txt (line 10)) (12.1.105)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2->-r requirements.txt (line 10)) (2.1.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb==0.16.2->-r requirements.txt (line 12)) (8.2.1)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb==0.16.2->-r requirements.txt (line 12)) (3.1.45)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb==0.16.2->-r requirements.txt (line 12)) (2.32.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb==0.16.2->-r requirements.txt (line 12)) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb==0.16.2->-r requirements.txt (line 12)) (2.34.1)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb==0.16.2->-r requirements.txt (line 12)) (0.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from wandb==0.16.2->-r requirements.txt (line 12)) (6.0.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb==0.16.2->-r requirements.txt (line 12)) (1.3.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb==0.16.2->-r requirements.txt (line 12)) (75.2.0)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.11/dist-packages (from wandb==0.16.2->-r requirements.txt (line 12)) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb==0.16.2->-r requirements.txt (line 12)) (4.25.8)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.2->-r requirements.txt (line 10)) (12.5.82)\n",
            "Requirement already satisfied: munch in /usr/local/lib/python3.11/dist-packages (from pretrainedmodels==0.7.4->segmentation_models_pytorch==0.3.3->-r requirements.txt (line 8)) (4.0.0)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (from timm==0.9.2->segmentation_models_pytorch==0.3.3->-r requirements.txt (line 8)) (0.34.3)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm==0.9.2->segmentation_models_pytorch==0.3.3->-r requirements.txt (line 8)) (0.6.1)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image->-r requirements.txt (line 13)) (2.37.0)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image->-r requirements.txt (line 13)) (25.0)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->-r requirements.txt (line 13)) (0.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 14)) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 14)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 14)) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 14)) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 14)) (3.2.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb==0.16.2->-r requirements.txt (line 12)) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb==0.16.2->-r requirements.txt (line 12)) (4.0.12)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb==0.16.2->-r requirements.txt (line 12)) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb==0.16.2->-r requirements.txt (line 12)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb==0.16.2->-r requirements.txt (line 12)) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb==0.16.2->-r requirements.txt (line 12)) (2025.8.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.1.2->-r requirements.txt (line 10)) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.1.2->-r requirements.txt (line 10)) (1.3.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb==0.16.2->-r requirements.txt (line 12)) (5.0.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->timm==0.9.2->segmentation_models_pytorch==0.3.3->-r requirements.txt (line 8)) (1.1.7)\n",
            "Using cached segmentation_models_pytorch-0.3.3-py3-none-any.whl (106 kB)\n",
            "Using cached timm-0.9.2-py3-none-any.whl (2.2 MB)\n",
            "Installing collected packages: timm, segmentation_models_pytorch\n",
            "  Attempting uninstall: timm\n",
            "    Found existing installation: timm 0.4.12\n",
            "    Uninstalling timm-0.4.12:\n",
            "      Successfully uninstalled timm-0.4.12\n",
            "  Attempting uninstall: segmentation_models_pytorch\n",
            "    Found existing installation: segmentation-models-pytorch 0.3.1\n",
            "    Uninstalling segmentation-models-pytorch-0.3.1:\n",
            "      Successfully uninstalled segmentation-models-pytorch-0.3.1\n",
            "Successfully installed segmentation_models_pytorch-0.3.3 timm-0.9.2\n",
            "Collecting segmentation-models-pytorch==0.3.1\n",
            "  Using cached segmentation_models_pytorch-0.3.1-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch==0.3.1) (0.16.2)\n",
            "Requirement already satisfied: pretrainedmodels==0.7.4 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch==0.3.1) (0.7.4)\n",
            "Requirement already satisfied: efficientnet-pytorch==0.7.1 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch==0.3.1) (0.7.1)\n",
            "Collecting timm==0.4.12 (from segmentation-models-pytorch==0.3.1)\n",
            "  Using cached timm-0.4.12-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch==0.3.1) (4.65.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch==0.3.1) (11.3.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from efficientnet-pytorch==0.7.1->segmentation-models-pytorch==0.3.1) (2.1.2)\n",
            "Requirement already satisfied: munch in /usr/local/lib/python3.11/dist-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch==0.3.1) (4.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision>=0.5.0->segmentation-models-pytorch==0.3.1) (1.24.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchvision>=0.5.0->segmentation-models-pytorch==0.3.1) (2.32.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch==0.3.1) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch==0.3.1) (4.14.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch==0.3.1) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch==0.3.1) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch==0.3.1) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch==0.3.1) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch==0.3.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch==0.3.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch==0.3.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch==0.3.1) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch==0.3.1) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch==0.3.1) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch==0.3.1) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch==0.3.1) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch==0.3.1) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch==0.3.1) (2.18.1)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch==0.3.1) (12.1.105)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch==0.3.1) (2.1.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch==0.3.1) (12.5.82)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch==0.3.1) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch==0.3.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch==0.3.1) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch==0.3.1) (2025.8.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch==0.3.1) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch==0.3.1) (1.3.0)\n",
            "Using cached segmentation_models_pytorch-0.3.1-py3-none-any.whl (102 kB)\n",
            "Using cached timm-0.4.12-py3-none-any.whl (376 kB)\n",
            "Installing collected packages: timm, segmentation-models-pytorch\n",
            "  Attempting uninstall: timm\n",
            "    Found existing installation: timm 0.9.2\n",
            "    Uninstalling timm-0.9.2:\n",
            "      Successfully uninstalled timm-0.9.2\n",
            "  Attempting uninstall: segmentation-models-pytorch\n",
            "    Found existing installation: segmentation-models-pytorch 0.3.3\n",
            "    Uninstalling segmentation-models-pytorch-0.3.3:\n",
            "      Successfully uninstalled segmentation-models-pytorch-0.3.3\n",
            "Successfully installed segmentation-models-pytorch-0.3.1 timm-0.4.12\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.16.2)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.2.1)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.45)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.34.1)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.11/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.25.8)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.8.3)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "W&B offline. Running your script from this directory will only write metadata locally. Use wandb disabled to completely turn off W&B.\n"
          ]
        }
      ],
      "source": [
        "# Clone MEDIAR github repo\n",
        "!git clone https://github.com/samik1986/Mediar_4_devBrain.git\n",
        "%rm -rf ./sample_data\n",
        "%cd Mediar_4_devBrain\n",
        "\n",
        "# # Install dependencies\n",
        "%pip install -r requirements.txt\n",
        "%pip install segmentation-models-pytorch==0.3.1\n",
        "%pip install wandb\n",
        "!wandb off"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install safetensors==0.3.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kriMuSe4iHxG",
        "outputId": "1906584a-8f4d-4b2e-f673-eef00902d66d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting safetensors==0.3.1\n",
            "  Downloading safetensors-0.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
            "Downloading safetensors-0.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: safetensors\n",
            "  Attempting uninstall: safetensors\n",
            "    Found existing installation: safetensors 0.6.1\n",
            "    Uninstalling safetensors-0.6.1:\n",
            "      Successfully uninstalled safetensors-0.6.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "accelerate 1.9.0 requires safetensors>=0.4.3, but you have safetensors 0.3.1 which is incompatible.\n",
            "transformers 4.55.0 requires safetensors>=0.4.3, but you have safetensors 0.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed safetensors-0.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvKYRYitHRNR"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### 2. Prepare MEDIAR Models\n",
        "\n",
        "MEDIAR uses two pretrained models to conduct ensemble prediction.\n",
        "\n",
        "- Model1 : fine-tuned from \"Phase 1\" pretraining.\n",
        "- Model2 : fine-tuned from \"Phase 2\" pretraining.\n",
        "\n",
        "for the details on how each model is trained, [please refer to our Paper](https://arxiv.org/abs/2212.03465).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "doVGxk2VHApv",
        "outputId": "26536ba0-f673-414f-a905-797f2ac6bf8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=168MtudjTMLoq9YGTyoD2Rjl_d3Gy6c_L\n",
            "From (redirected): https://drive.google.com/uc?id=168MtudjTMLoq9YGTyoD2Rjl_d3Gy6c_L&confirm=t&uuid=dffb6e91-903c-4c64-9666-cba9e554a403\n",
            "To: /content/Mediar_4_devBrain/weights/from_phase1.pth\n",
            "100% 486M/486M [00:07<00:00, 69.1MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1JJ2-QKTCk-G7sp5ddkqcifMxgnyOrXjx\n",
            "From (redirected): https://drive.google.com/uc?id=1JJ2-QKTCk-G7sp5ddkqcifMxgnyOrXjx&confirm=t&uuid=e3a19af9-12bd-4102-a81c-354757310295\n",
            "To: /content/Mediar_4_devBrain/weights/from_phase2.pth\n",
            "100% 486M/486M [00:05<00:00, 84.6MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Download MEDIAR pretrained weights\n",
        "%mkdir weights\n",
        "!gdown https://drive.google.com/uc?id=168MtudjTMLoq9YGTyoD2Rjl_d3Gy6c_L -O weights/from_phase1.pth\n",
        "!gdown https://drive.google.com/uc?id=1JJ2-QKTCk-G7sp5ddkqcifMxgnyOrXjx -O weights/from_phase2.pth\n",
        "\n",
        "model_path1 = \"./weights/from_phase1.pth\"\n",
        "model_path2 = \"./weights/from_phase2.pth\"\n",
        "\n",
        "import torch\n",
        "weights1 = torch.load(model_path1, map_location=\"cpu\")\n",
        "weights2 = torch.load(model_path2, map_location=\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOk64L32L9Xa",
        "outputId": "00b610a3-9c8b-4a50-b38b-b3f1e368107c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=168MtudjTMLoq9YGTyoD2Rjl_d3Gy6c_L\n",
            "From (redirected): https://drive.google.com/uc?id=168MtudjTMLoq9YGTyoD2Rjl_d3Gy6c_L&confirm=t&uuid=270e152d-13a0-478e-9405-ccf09d2c8e0e\n",
            "To: /content/Mediar_4_devBrain/weights/pretrained/phase1.pth\n",
            "100% 486M/486M [00:03<00:00, 159MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1JJ2-QKTCk-G7sp5ddkqcifMxgnyOrXjx\n",
            "From (redirected): https://drive.google.com/uc?id=1JJ2-QKTCk-G7sp5ddkqcifMxgnyOrXjx&confirm=t&uuid=6649dddb-8334-4ea1-af3e-f4c68b99a549\n",
            "To: /content/Mediar_4_devBrain/weights/pretrained/phase2.pth\n",
            "100% 486M/486M [00:06<00:00, 78.7MB/s]\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p weights/pretrained\n",
        "!gdown https://drive.google.com/uc?id=168MtudjTMLoq9YGTyoD2Rjl_d3Gy6c_L -O weights/pretrained/phase1.pth\n",
        "!gdown https://drive.google.com/uc?id=1JJ2-QKTCk-G7sp5ddkqcifMxgnyOrXjx -O weights/pretrained/phase2.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "54UrNQQXm4ny",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bb71d1d-0280-46aa-e396-f214a4bce79a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mv: cannot stat '/content/weights/pretrained/phase1.pth': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "%mkdir -p /content/MEDIAR/weights/pretrained\n",
        "%mv /content/weights/pretrained/phase1.pth /content/MEDIAR/weights/pretrained/phase1.pth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AciCYfy2TFyF"
      },
      "source": [
        "We load weights on the our `MEDIARFormer` model.\n",
        "\n",
        "`MEDIARFormer` predicts 3-dimensional outputs (3 classes), where each corresponds to:\n",
        "\n",
        "- `Cell Recognition`: Predicts whether a pixel belongs to `cell (1)` or `background (0)`\n",
        "- `Cell Distinction`: Horizontal Vector & Vertical Vector to the cell center."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2iwnqsUQvLn",
        "outputId": "43e70812-d05a-4cb7-8d49-9473b14448a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/qubvel/segmentation_models.pytorch/releases/download/v0.0.2/mit_b5.pth\" to /root/.cache/torch/hub/checkpoints/mit_b5.pth\n",
            "100%|██████████| 313M/313M [00:01<00:00, 172MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "import glob\n",
        "import skimage.io as io\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from train_tools import *\n",
        "from train_tools.models import MEDIARFormer\n",
        "from core.MEDIAR import Predictor, EnsemblePredictor\n",
        "\n",
        "model_args = {\n",
        "    \"classes\": 3,\n",
        "    \"decoder_channels\": [1024, 512, 256, 128, 64],\n",
        "    \"decoder_pab_channels\": 256,\n",
        "    \"encoder_name\": 'mit_b5',\n",
        "    \"in_channels\": 3\n",
        "}\n",
        "\n",
        "model1 = MEDIARFormer(**model_args)\n",
        "model1.load_state_dict(weights1, strict=False)\n",
        "\n",
        "model2 = MEDIARFormer(**model_args)\n",
        "model2.load_state_dict(weights2, strict=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuyAtIkgDWah"
      },
      "source": [
        "Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAWg-SB7FlqD",
        "outputId": "027b00e1-da1d-43fb-feb8-3a4131385995"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm train_tools/data_utils/mapping_labeled.json\n",
        "!rm train_tools/data_utils/mapping_tuning.json\n",
        "!rm train_tools/data_utils/mapping_public.json"
      ],
      "metadata": {
        "id": "jTYVACkUuXLO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "import re\n",
        "\n",
        "def natural_sort_key(s):\n",
        "    return [int(text) if text.isdigit() else text.lower()\n",
        "            for text in re.split(r'(\\d+)', s)]\n",
        "\n",
        "images_raw = sorted(glob.glob(image_path), key=natural_sort_key)\n",
        "labels_raw = sorted(glob.glob(label_path), key=natural_sort_key)\n"
      ],
      "metadata": {
        "id": "HebrEhJaDBDh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OI0R-1P2DYKp",
        "outputId": "2a6b5fc8-161c-4f4e-b2c3-9801a61468fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "----------- Path Mapping for Labeled Data is Started... -----------\n",
            "\n",
            "\n",
            "----------- Path Mapping for Tuning Data is Started... -----------\n",
            "\n",
            "\n",
            "----------- Path Mapping for Public Data is Started... -----------\n",
            "\n",
            "\n",
            "-------------- Path Mapping is Ended !!! ---------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!python ./generate_mapping.py --root=/content/drive/MyDrive/Root"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3oFuiU1oj64",
        "outputId": "1d7f65c6-eea9-43e6-8cf1-8f9db123e9fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting imagecodecs\n",
            "  Downloading imagecodecs-2025.8.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from imagecodecs) (1.24.3)\n",
            "Downloading imagecodecs-2025.8.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (26.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.4/26.4 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: imagecodecs\n",
            "Successfully installed imagecodecs-2025.8.2\n"
          ]
        }
      ],
      "source": [
        "!pip install imagecodecs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "rFMnyQa3pZo9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4Zj2dRIDrL9",
        "outputId": "c6e10872-5bd1-48bd-e594-7f5d0af38a38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-08-11 20:15:25.752142: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1754943325.772940    4985 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1754943325.779102    4985 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1754943325.794832    4985 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754943325.794859    4985 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754943325.794863    4985 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754943325.794867    4985 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\n",
            "================================================== Configuration ==================================================\n",
            "{'data_setups': {'labeled': {'amplified': False,\n",
            "                             'batch_size': 1,\n",
            "                             'mapping_file': './train_tools/data_utils/mapping_labeled.json',\n",
            "                             'mapping_file_tuning': '/content/Mediar_4_devBrain/train_tools/data_utils/mapping_tuning.json',\n",
            "                             'root': '/content/Mediar_4_devBrain/',\n",
            "                             'valid_portion': 0.0},\n",
            "                 'public': {'enabled': False,\n",
            "                            'params': {'batch_size': 1,\n",
            "                                       'mapping_file': './train_tools/data_utils/mapping_public.json',\n",
            "                                       'root': '/content/Mediar_4_devBrain/'}},\n",
            "                 'unlabeled': {'enabled': False}},\n",
            " 'pred_setups': {'algo_params': {'use_tta': False},\n",
            "                 'exp_name': 'mediar_from_phase1',\n",
            "                 'input_path': '/content/Mediar_4_devBrain/data/Official/Tuning/images',\n",
            "                 'make_submission': True,\n",
            "                 'output_path': './results/'},\n",
            " 'train_setups': {'model': {'name': 'mediar-former',\n",
            "                            'params': {},\n",
            "                            'pretrained': {'enabled': True,\n",
            "                                           'strict': False,\n",
            "                                           'weights': './weights/pretrained/phase1.pth'}},\n",
            "                  'optimizer': {'name': 'adamw', 'params': {'lr': 2e-05}},\n",
            "                  'scheduler': {'enabled': True,\n",
            "                                'name': 'cosine',\n",
            "                                'params': {'T_max': 100, 'eta_min': 1e-07}},\n",
            "                  'seed': 19940817,\n",
            "                  'trainer': {'name': 'mediar',\n",
            "                              'params': {'algo_params': {'with_public': False},\n",
            "                                         'amp': True,\n",
            "                                         'device': 'cuda:0',\n",
            "                                         'num_epochs': 100,\n",
            "                                         'valid_frequency': 1}}},\n",
            " 'wandb_setups': {'group': 'Fine-tuning',\n",
            "                  'name': 'from_phase1',\n",
            "                  'project': 'CellSeg'}}\n",
            "===================================================================================================================\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B syncing is set to \u001b[1m`offline`\u001b[0m in this directory.  \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb online`\u001b[0m or set \u001b[1mWANDB_MODE=online\u001b[0m to enable cloud syncing.\n",
            "\n",
            "Loading pretrained model....\n",
            "\n",
            "(DataLoaded) Training data size: 164, Validation data size: 0\n",
            "\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n",
            "\n",
            "==================================================\n",
            "Train start on device: Tesla T4\n",
            "==================================================\n",
            "[Round 1/100]\n",
            ">>> Train Epoch\n",
            "100% 164/164 [01:41<00:00,  1.61it/s]\n",
            "{'Train_Dice_Loss': 0.1867}\n",
            "\n",
            ">>> TuningSet Epoch\n",
            "100% 70/70 [00:51<00:00,  1.36it/s]\n",
            "Cell Counts Total: (6963)\n",
            "{'TuningSet_Cell_Count': 6963}\n",
            "\n",
            "\n",
            ">>>> Update Best Model with score: 6963\n",
            "\n",
            "--------------------------------------------------\n",
            "[Round 2/100]\n",
            ">>> Train Epoch\n",
            "100% 164/164 [01:35<00:00,  1.72it/s]\n",
            "{'Train_Dice_Loss': 0.1699}\n",
            "\n",
            ">>> TuningSet Epoch\n",
            "100% 70/70 [00:20<00:00,  3.47it/s]\n",
            "Cell Counts Total: (6946)\n",
            "{'TuningSet_Cell_Count': 6946}\n",
            "\n",
            "\n",
            ">>>> Update Best Model with score: 6946\n",
            "\n",
            "--------------------------------------------------\n",
            "[Round 3/100]\n",
            ">>> Train Epoch\n",
            "100% 164/164 [01:37<00:00,  1.69it/s]\n",
            "{'Train_Dice_Loss': 0.1644}\n",
            "\n",
            ">>> TuningSet Epoch\n",
            "100% 70/70 [00:16<00:00,  4.27it/s]\n",
            "Cell Counts Total: (5717)\n",
            "{'TuningSet_Cell_Count': 5717}\n",
            "\n",
            "\n",
            ">>>> Update Best Model with score: 5717\n",
            "\n",
            "--------------------------------------------------\n",
            "[Round 4/100]\n",
            ">>> Train Epoch\n",
            " 72% 118/164 [01:09<00:33,  1.39it/s]"
          ]
        }
      ],
      "source": [
        "!python ./main.py --config_path=/content/Mediar_4_devBrain/config/step2_finetuning/finetuning1.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azQ19p4ZaAzB"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "### 3. Prepare Microscopy Data\n",
        "\n",
        "We prepared 2 microscopy images in the `./input_path` directory. Let's see the images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "gAZOo3isZqzc",
        "outputId": "06d673df-d231-4909-957a-3b1c3a830b94"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'io' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-378417699.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minput_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./image/examples\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mimg1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{input_path}/img1.tiff\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'io' is not defined"
          ]
        }
      ],
      "source": [
        "input_path = \"./image/examples\"\n",
        "\n",
        "img1 = io.imread(f\"{input_path}/img1.tiff\") #\n",
        "io.imshow(img1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yIbBr0rkMlVT"
      },
      "outputs": [],
      "source": [
        "# img2 = io.imread(f\"{input_path}/cell3.png\") #\n",
        "# io.imshow(img2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZTpyxVUOVuj"
      },
      "outputs": [],
      "source": [
        "# img3 = io.imread(f\"{input_path}/brain cells.png\") #\n",
        "# io.imshow(img3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbFaAGYoz9V5"
      },
      "source": [
        "# 새 섹션"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnZBiCBLMfA0"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "### 4. Prediction - (1): Single Model\n",
        "\n",
        "Let's detect the cell instances in the image using a single `MEDIARFormer` model.\n",
        "\n",
        "- The `Predictor` conduct its pediction on all images in the `input_path` and save the results in the `output_path`.\n",
        "\n",
        "- In this example, we do not use test-time Augmentation by setting `use_tta` as `False`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7JH4RXWLbbO",
        "outputId": "63474403-8df3-4917-e4a4-bf1d30d1a2b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction finished: img1.tiff; img size = torch.Size([1, 3, 480, 640]); costing: 1.23s\n",
            "Prediction finished: img2.tif; img size = torch.Size([1, 3, 2048, 2048]); costing: 12.43s\n",
            "\n",
            " Total Time Cost: 13.65s\n"
          ]
        }
      ],
      "source": [
        "output_path = \"results\"\n",
        "\n",
        "predictor = Predictor(model1, \"cuda:0\", input_path, output_path, algo_params={\"use_tta\": False})\n",
        "_ = predictor.conduct_prediction()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnFJZP35Mcx4"
      },
      "source": [
        "The results are as follows:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.io import imread\n",
        "from skimage.color import label2rgb\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score\n",
        "import seaborn as sns\n",
        "\n",
        "# Define folders\n",
        "image_folder = \"/content/MEDIAR/data/Official/Tuning/images\"\n",
        "gt_folder = \"/content/MEDIAR/data/Official/Tuning/labels\"\n",
        "pred_folder = \"/content/MEDIAR/results\"\n",
        "\n",
        "# Helper to match predicted labels to ground truth using Hungarian algorithm\n",
        "def match_labels(gt, pred):\n",
        "    gt_labels = np.unique(gt)\n",
        "    pred_labels = np.unique(pred)\n",
        "\n",
        "    gt_labels = gt_labels[gt_labels != 0]\n",
        "    pred_labels = pred_labels[pred_labels != 0]\n",
        "\n",
        "    cost_matrix = np.zeros((len(gt_labels), len(pred_labels)))\n",
        "\n",
        "    for i, g in enumerate(gt_labels):\n",
        "        g_mask = gt == g\n",
        "        for j, p in enumerate(pred_labels):\n",
        "            p_mask = pred == p\n",
        "            intersection = np.logical_and(g_mask, p_mask).sum()\n",
        "            union = np.logical_or(g_mask, p_mask).sum()\n",
        "            iou = intersection / union if union != 0 else 0\n",
        "            cost_matrix[i, j] = -iou  # Negative for maximization\n",
        "\n",
        "    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
        "\n",
        "    mapping = {}\n",
        "    for r, c in zip(row_ind, col_ind):\n",
        "        if -cost_matrix[r, c] > 0.1:  # Threshold to ignore bad matches\n",
        "            mapping[pred_labels[c]] = gt_labels[r]\n",
        "\n",
        "    matched_pred = np.zeros_like(pred)\n",
        "    for pl, gl in mapping.items():\n",
        "        matched_pred[pred == pl] = gl\n",
        "\n",
        "    return matched_pred\n",
        "\n",
        "# Compute confusion for foreground/background\n",
        "def compute_confusion(y_true, y_pred):\n",
        "    binary_true = y_true > 0\n",
        "    binary_pred = y_pred > 0\n",
        "\n",
        "    tp = np.logical_and(binary_true, binary_pred).sum()\n",
        "    tn = np.logical_and(~binary_true, ~binary_pred).sum()\n",
        "    fp = np.logical_and(~binary_true, binary_pred).sum()\n",
        "    fn = np.logical_and(binary_true, ~binary_pred).sum()\n",
        "\n",
        "    total = tp + tn + fp + fn\n",
        "    return {\n",
        "        \"TP\": tp / total,\n",
        "        \"FP\": fp / total,\n",
        "        \"FN\": fn / total,\n",
        "        \"TN\": tn / total\n",
        "    }\n",
        "\n",
        "# Store all metrics\n",
        "all_confusions = []\n",
        "all_precisions, all_recalls, all_f1s, all_accuracies = [], [], [], []\n",
        "\n",
        "# Find matching files\n",
        "files = sorted([f for f in os.listdir(image_folder) if f.endswith('.tiff')])\n",
        "\n",
        "# Plotting\n",
        "fig, axs = plt.subplots(len(files), 3, figsize=(12, 4 * len(files)))\n",
        "\n",
        "for idx, fname in enumerate(files):\n",
        "    image_path = os.path.join(image_folder, fname)\n",
        "    gt_path = os.path.join(gt_folder, fname.replace('.tiff', '_label.tiff'))\n",
        "    pred_path = os.path.join(pred_folder, fname.replace('.tiff', '_label.tiff'))\n",
        "\n",
        "    img = imread(image_path)\n",
        "    gt = imread(gt_path)\n",
        "    pred = imread(pred_path)\n",
        "\n",
        "    pred_matched = match_labels(gt, pred)\n",
        "\n",
        "    # Compute confusion\n",
        "    confusion = compute_confusion(gt, pred_matched)\n",
        "    all_confusions.append(confusion)\n",
        "\n",
        "    # Flatten for metric calculation\n",
        "    y_true = (gt > 0).flatten().astype(int)\n",
        "    y_pred = (pred_matched > 0).flatten().astype(int)\n",
        "\n",
        "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
        "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
        "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "\n",
        "    all_precisions.append(precision)\n",
        "    all_recalls.append(recall)\n",
        "    all_f1s.append(f1)\n",
        "    all_accuracies.append(acc)\n",
        "\n",
        "    # Plot\n",
        "    axs[idx, 0].imshow(img, cmap='gray')\n",
        "    axs[idx, 0].set_title(f\"Original - {fname}\")\n",
        "    axs[idx, 0].axis('off')\n",
        "\n",
        "    axs[idx, 1].imshow(label2rgb(pred_matched, image=img, bg_label=0))\n",
        "    axs[idx, 1].set_title(\"Predicted Mask\")\n",
        "    axs[idx, 1].axis('off')\n",
        "\n",
        "    axs[idx, 2].imshow(label2rgb(gt, image=img, bg_label=0))\n",
        "    axs[idx, 2].set_title(\"Ground Truth Mask\")\n",
        "    axs[idx, 2].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ---- Plot averaged confusion matrix ----\n",
        "avg_conf = {\n",
        "    k: np.mean([conf[k] for conf in all_confusions])\n",
        "    for k in ['TP', 'FP', 'FN', 'TN']\n",
        "}\n",
        "\n",
        "conf_matrix = np.array([\n",
        "    [avg_conf['TP'], avg_conf['FP']],\n",
        "    [avg_conf['FN'], avg_conf['TN']]\n",
        "])\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(conf_matrix, annot=True, cmap='Blues', xticklabels=['Pred: Obj', 'Pred: Bkg'], yticklabels=['True: Obj', 'True: Bkg'], fmt=\".2f\")\n",
        "plt.title(\"Average Confusion Matrix (Fraction)\")\n",
        "plt.show()\n",
        "\n",
        "# ---- Print average metrics ----\n",
        "print(f\"Average Precision: {np.mean(all_precisions):.4f}\")\n",
        "print(f\"Average Recall: {np.mean(all_recalls):.4f}\")\n",
        "print(f\"Average F1 Score: {np.mean(all_f1s):.4f}\")\n",
        "print(f\"Average Accuracy: {np.mean(all_accuracies):.4f}\")\n"
      ],
      "metadata": {
        "id": "zlmJU3Mu5AhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgLEVrr1svf1"
      },
      "outputs": [],
      "source": [
        "# pred2 = io.imread(f\"./{output_path}/brain cells_label.tiff\")\n",
        "# io.imshow(pred2, cmap=\"cividis\")\n",
        "\n",
        "# cell_count = len(np.unique(pred2))-1 # exclude the background\n",
        "# print(f\"\\n{cell_count} Cells detected!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qj4SC8reOq7S"
      },
      "outputs": [],
      "source": [
        "pred2 = io.imread(f\"./{output_path}/cell3_label.tiff\")\n",
        "io.imshow(pred2, cmap=\"cividis\")\n",
        "\n",
        "cell_count = len(np.unique(pred2))-1 # exclude the background\n",
        "print(f\"\\n{cell_count} Cells detected!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYH3pAxlPFky"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "### 5. Prediction - (2): Ensemble Model + TTA\n",
        "\n",
        "Let's use the ensemble models with TTA.\n",
        "\n",
        "- In this example, we use test-time Augmentation by setting `use_tta` as `True`.\n",
        "\n",
        "It takes much longer, as it need to conduct multiple forward paths."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYoKqp_ms_ha"
      },
      "outputs": [],
      "source": [
        "predictor = EnsemblePredictor(model1, model2, \"cuda:0\", input_path, output_path, algo_params={\"use_tta\": True})\n",
        "_ = predictor.conduct_prediction()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yu-8gyX9NZg"
      },
      "outputs": [],
      "source": [
        "pred1 = io.imread(f\"./{output_path}/img1_label.tiff\")\n",
        "io.imshow(pred1, cmap=\"cividis\")\n",
        "\n",
        "cell_count = len(np.unique(pred1))-1 # exclude the background\n",
        "print(f\"\\n{cell_count} Cells detected!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2z10cEjsPsTr"
      },
      "outputs": [],
      "source": [
        "pred2 = io.imread(f\"./{output_path}/img2_label.tiff\")\n",
        "io.imshow(pred2, cmap=\"cividis\")\n",
        "\n",
        "cell_count = len(np.unique(pred2))-1 # exclude the background\n",
        "print(f\"\\n{cell_count} Cells detected!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gc1HjroJAO7D"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ea37095",
        "outputId": "407ceabb-7942-4850-83e3-4efb8aaa23e4"
      },
      "source": [
        "import torch\n",
        "print(torch.__version__)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.1.2+cu121\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "azQ19p4ZaAzB",
        "AnZBiCBLMfA0",
        "bYH3pAxlPFky"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}